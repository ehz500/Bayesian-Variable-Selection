<<<<<<< HEAD
library("BRugs")
install.packages("BRugs")
install.packages("BRugs")
library(iBUGS)
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
tau ~ dgamma(1.0E-4,1.0E-4)
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20)
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind [i] <- IndA[i]-1
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
for(i in 1:n){
yest[i]~dbin(pi[i],1)
}
}"
, file="ssvs_03.txt")
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
modelo_jags1<-1
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
View(tabla_entrena2)
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM","yest")
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
for(i in 1:n){
yest[i]~dbin(pi[i],1)
}
}"
, file="ssvs_03.txt")
out2 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out2))
probs <- out2$summary$statistics
?"run.JAGS"
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
inits<-function(){list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM","yest")
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters,inits=list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n)), data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters,inits=list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n)), data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
n
var_expl
rep(1,var_expl)
IndA
IndA=c(rep(1,var_expl))
IndA
out3 <- run.jags("ssvs_03.txt", parameter, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
initlist <- replicate(8,list(m=runif(1,-20,20)),simplify=FALSE)
initlist
inits<-function(){list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
inits
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
inits()
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
#for(i in 1:n){
#yest[i]~dbin(pi[i],1)
#}
}"
, file="ssvs_03.txt")
inits<-function(){list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits,
method="parallel", adapt=5000, burnin=5000)
c(rep(1,var_expl))
failed.jags()
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)))}
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out2$summary$statistics
probs <- out3$summary$statistics
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
#-Defining inits-
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out3$summary$statistics
View(probs)
ncov <- 20
nobs <- 60
var_beta <- .004
c <- 1000
p_inclusion <- .5
sigma_y <- 1
# generate covariates
X <- array(dim=c(nobs, ncov))
for (i in 1:ncov){
X[, i] <- rnorm(nobs, 0, 1)
}
included <- rbinom(ncov, 1, p_inclusion)
coefs <- rnorm(n=ncov,
mean=0,
sd=ifelse(included==1,
sqrt(var_beta * c),
sqrt(var_beta)
)
)
coefs <- sort(coefs)
Y <- rnorm(nobs, mean=X %*% coefs, sd=sigma_y)
require(gridExtra)
require(runjags)
require(ggmcmc)
library(coda)
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
out2 <- run.jags("ssvs.txt", vars, data=dat, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf2 <- ggs(as.mcmc.list(out2))
probs2 <- out2$summary$statistics[((3):(2+ncov)), 1]
probs2 <- out2$summary$statistics
View(probs2)
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
out2 <- run.jags("ssvs.txt", vars, data=dat, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out2))
probs <- summary(out2)$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- summary$out2$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- summary$out2$statistics
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
outdf <- ggs(as.mcmc.list(out2))
probs <- summary$out2$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- out2$summary$statistics
library(Rcpp)
library(maps)
library(mapproj)
library(ggplot2)
library(shiny)
library(R2jags)
library(rjags)
library(psych)
library(caret)
library(bnclassify)
library(klaR)
library(rocc)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/Modelos_Lineales_Generalizados/Proyecto_equipo")
plot_proporcion_Ausentismo<-read.csv("plot_proporcion_Ausentismo.csv",header=TRUE)
plot_cantidad_Ausentismo<-read.csv("plot_cantidad_Ausentismo.csv",header=TRUE)
plot_cantidad_Casillas<-read.csv("plot_cantidad_Casillas.csv",header=TRUE)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/Modelos_Lineales_Generalizados/Proyecto_equipo")
tabla<-read.csv("x_4_2_Solo5entidades_yDF.csv",header=TRUE)
modelo_binomial_logit<-function(){
shiny::runApp('~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/EstadisticaComputacional/Proyecto_final_c2c_5entidades')
shiny::runApp('~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/EstadisticaComputacional/Proyecto_final_c2c_5entidades')
out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
View(probs)
data.frame(coefs, pos = 1:ncov)
X %*% coefs
labels <- rep(NA, ncov)
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
labels <- rep(NA, ncov)
labels
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
labels
xdf <- data.frame(Parameter = labels, value = 1:ncov)
View(probs)
View(probs2)
xdf
or_x<-order(abs(x))
or_x<-order(abs(coef))
or_x<-order(abs(coefs))
or_coef<-order(abs(coefs))
p2 <- ggplot(df, aes(x=abs(coefs)[or_coef], y=probs[or_coef])) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
p2
ggplot(df, aes(x=abs(coefs)[or_coef], y=probs[or_coef])) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
df
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
out2$summary$statistics[((3):(2+ncov)), 1]
probs2 <- out2$summary$statistics[((3):(2+ncov)), 1]
labels <- rep(NA, ncov)
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
xdf <- data.frame(Parameter = labels, value = 1:ncov)
or_coef<-order(abs(coefs))
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
p2 <- ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
?rbinom
rbinom(ncov, 1, p_inclusion)
?ifelse
rnorm(nobs, mean=X %*% coefs, sd=sigma_y)
summary(out2)
out2$summary$statistics
coefs
outdf
or_coef
abs(coefs)
order(abs(coefs))
?order
cfs<-numeric(length(coefs))
for(i in 1:length(coefs)){
cfs[i]<-coefs[i]
}
cfs
abs_cfs<-numeric(length(coefs))
abs_cfs<-numeric(length(coefs))
for(i in 1:length(coefs)){
cfs[i]<-coefs[i]
abs_cfs[i]<-abs(coefs[i])
}
abs_cfs
sort(abs_cfs)
order(abs_cfs)
or1<-order(abs_cfs)
ordenados<-abs_cfs[or1]
ordenados
ordenados_proba<-probs[or_coef]
ordenados_proba
probs
probs <- out2$summary$statistics[((3):(2+ncov)), 1]
probs
or_coef
abs(coefs)[or_coef]
probs[or_coef]
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
#for(i in 1:n){
#yest[i]~dbin(pi[i],1)
#}
}"
, file="ssvs_03.txt")
out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
require(gridExtra)
require(runjags)
require(ggmcmc)
library(coda)
=======
tofdg <- function(matriz){
a <- as.data.frame(matrix(0,nrow=dim(matriz)[1]**2/2,ncol=3))
contador <- 0
for(i in 1:dim(matriz)[2]){
col <- matriz[,i]
for(j in 1:i){
a[contador+1,3] <- as.numeric(matriz[i,j])
a[contador+1,2] <- j
a[contador+1,1] <- i
contador <- contador +1
}
}
a
}
x <- tofdg(adj.matrix)
z <- as.data.frame(cbind(seq(1,clusters,1),t(summary_cluster)))
colores2 <- as.data.frame(cbind(seq(1,clusters,1),t(colores)[1:clusters]))
qrage(links=x, width = 1000, height = 800,distance=8000,nodeValue=z
,nodeColor=colores2,linkColor='#00f',arrowColor='#f00'
,cut=0.01,textSize=12
,linkWidth=c(1,8),linkOpacity=c(0.6,1))
View(df_)
View(df)
browseURL("file:////Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html")
?fromJSON
library(jsonlite)
?fromJSON
mat <- matrix(c(1:3,7:9,4:6),byrow=T,nc=3)
which.max( mat[,2] )
mat[,2]
mat[,2]
which.max( mat[,2] )
which.min( mat[,2] )
library(fpc)
library(dbscan)
library(kernlab)
library(jsonlite)
library(igraph)
library(RColorBrewer)
library(qrage)
library(psych)
>>>>>>> 7b263f4b9c1202cc4e925a5b8deb8145883bf996
library(Rcpp)
library(maps)
library(mapproj)
library(ggplot2)
library(shiny)
library(R2jags)
library(rjags)
library(psych)
library(caret)
library(bnclassify)
library(klaR)
library(rocc)
<<<<<<< HEAD
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
# modelo_jags1<-1
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
#-Defining data-
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out3$summary$statistics[((3):(2+ncov)), 1]
ncov<-var_expl
probs <- out3$summary$statistics[((3):(2+ncov)), 1]
out3$summary$statistics
names(tabla)
names(tabla)[10:40]
vector_variables<-names(tabla)[10:40]
proporcion_entrena<-.6
vector_variables<-names(tabla)[10:40]
iteraciones_jags<-1000
calentamiento_jags<-200
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
var_expl
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
=======
#----------------------------- LOAD DATA -----------------------------
rm(list = ls())
#rutawork = ('/home/denny/itam/topologia/proyecto_clustering/')
rutawork = ('/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos/proyecto_clustering/')
datos <- read.csv(paste(rutawork,'ecobici_preprocessed.csv',sep = ""), header = TRUE, sep = ",", quote="\"", dec=".", fill = TRUE)
datos_origin<-datos
str(datos)
proporcion_entrena<-.01
inTraining <- createDataPartition(datos_origin$Genero_Usuario, p = proporcion_entrena, list = FALSE)
datos <- datos_origin[ inTraining,]
datos <- datos[c("Edad_Usuario", "Distancia_km", "Duracion_viaje")]
variables <- length(datos)
sigma <- 1
kres <- kpca(~., data=datos,features=variables,kernel="rbfdot",kpar = list(sigma = sigma))
data_kpca <- as.data.frame(kres@rotated)
#ordena de mayor a menor (por la que tiene mayor varianza)
datos_prueba <- data_kpca[c("V1","V2")]
#----------------------------- GENERATE INTERVALS----------------------------------
df <- datos_prueba      #choose a dataset
#----------------------------- NECESSARY PARAMETERS -----------------------------
#var_o <- data$x1    #variable we will use to make the overlapping subsets
var_o <- datos_prueba$V1   #if we want to use kernel pca variable to cut
n_int <- 6  #number of intervals we want
p <- 0.2  #proportion of each interval that should overlap with the next
#----------------------------- CREATING THE INTERVALS -----------------------------
#this section will create a data frame in which we will construct overlapping intervals
intervals_centers <- seq(min(var_o),max(var_o),length=n_int)  #basic partition = centers
interval_length <- intervals_centers[2]-intervals_centers[1]  #to create the overlaps of p% of this length
intervals <- data.frame(centers=intervals_centers)            #create a data frame
#create the overlapping intervals
intervals$min <- intervals_centers - (0.5+p)*interval_length
intervals$max <- intervals_centers + (0.5+p)*interval_length
intervals$interval <- seq(1,n_int)
intervals$name <- with(intervals, sprintf("[%.2f;%.2f)",min,max))
#function that will split the variable according to the invervals
res <- lapply(split(intervals,intervals$interval), function(x){
return(df[var_o> x$min & var_o <= x$max,])     #res will be a list with each element res[i]
})                                               #being the points on the i'th subset
df$Clusters<-numeric(nrow(df))
for(i in 1:length(res)){
# i<-1
interval_temp<-res[[i]]
distmat <- as.matrix(dist(interval_temp))
neigh <- 6
clusters_i<-clustITAM(distmat, neigh)
interval_temp$Cluster<-numeric(nrow(interval_temp))
for(j in 1:length(clusters_i)){
#j<-1
for(z in 1:length(clusters_i[[j]])){
#  z<-1
interval_temp$Cluster[as.numeric(clusters_i[[j]][z])]<-j
#df$Clusters[as.numeric(rownames(interval_temp)[nrow(interval_temp)])]<-j
}
}
res[[i]]<-interval_temp
}
for(i in 1:length(res)){
df[[ncol(df)+1]]<-numeric(nrow(df))
interval_temp<-res[[i]]
interval_temp$filas<-rownames(interval_temp)
for(j in 1:nrow(df)){
if(rownames(df)[j] %in% rownames(interval_temp)){
df[[ncol(df)]][j]<-interval_temp$Cluster[which(interval_temp$filas==rownames(df)[j])]
}#else{
#       df[[ncol(df)+1]][j]<-0
#     }
}
}
df_<-df
for(i in 1:length(res)){
ncol<-ncol(df_)
df_[[ncol+1]]<-numeric(nrow(df_))
ncol<-ncol(df_)
if(i==1){
contador_clusters<-0
df_[[ncol]]<-df_[[2+i]]
}else{
for(j in 1:nrow(df_)){
if(df_[[2+i]][j]!=0){
df_[[ncol]][j]<-df_[[2+i]][j]+contador_clusters
}
}
}
contador_clusters<-contador_clusters+max(df_[[2+i]])
}
df_salvado<-df
df<-df_[,-c(3:8)]
View(df_salvado)
View(df_)
library(fpc)
library(dbscan)
library(kernlab)
library(jsonlite)
library(igraph)
library(RColorBrewer)
library(qrage)
#----------------------------- LOAD DATA -----------------------------
rm(list = ls())
#rutawork = ('/home/denny/itam/topologia/proyecto_clustering/')
rutawork = ('/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos/proyecto_clustering/')
datos <- read.csv(paste(rutawork,'ecobici_preprocessed.csv',sep = ""), header = TRUE, sep = ",", quote="\"", dec=".", fill = TRUE)
datos_origin<-datos
datos<-datos_origin[c("Edad_Usuario", "Distancia_km", "Duracion_viaje")]
variables <- length(datos)
sigma <- 1
kres <- kpca(~., data=datos,features=variables,kernel="rbfdot",kpar = list(sigma = sigma))
print("WEEEEEEEEEY  YA QUEDÓ EL PINCHE PCA")
datos_prueba <- data_kpca[c("V1","V2")]
#----------------------------- GENERATE INTERVALS----------------------------------
df <- datos_prueba      #choose a dataset
#----------------------------- NECESSARY PARAMETERS -----------------------------
#var_o <- data$x1    #variable we will use to make the overlapping subsets
var_o <- datos_prueba$V1   #if we want to use kernel pca variable to cut
n_int <- 6  #number of intervals we want
p <- 0.2  #proportion of each interval that should overlap with the next
data_kpca <- as.data.frame(kres@rotated)
datos_prueba <- data_kpca[c("V1","V2")]
df <- datos_prueba      #choose a dataset
#----------------------------- NECESSARY PARAMETERS -----------------------------
#var_o <- data$x1    #variable we will use to make the overlapping subsets
var_o <- datos_prueba$V1   #if we want to use kernel pca variable to cut
n_int <- 6  #number of intervals we want
p <- 0.2  #proportion of each interval that should overlap with the next
#----------------------------- CREATING THE INTERVALS -----------------------------
#this section will create a data frame in which we will construct overlapping intervals
intervals_centers <- seq(min(var_o),max(var_o),length=n_int)  #basic partition = centers
interval_length <- intervals_centers[2]-intervals_centers[1]  #to create the overlaps of p% of this length
intervals <- data.frame(centers=intervals_centers)            #create a data frame
#create the overlapping intervals
intervals$min <- intervals_centers - (0.5+p)*interval_length
intervals$max <- intervals_centers + (0.5+p)*interval_length
intervals$interval <- seq(1,n_int)
intervals$name <- with(intervals, sprintf("[%.2f;%.2f)",min,max))
#function that will split the variable according to the invervals
res <- lapply(split(intervals,intervals$interval), function(x){
return(df[var_o> x$min & var_o <= x$max,])     #res will be a list with each element res[i]
})                                               #being the points on the i'th subset
df$Clusters<-numeric(nrow(df))
for(i in 1:length(res)){
# i<-1
interval_temp<-res[[i]]
distmat <- as.matrix(dist(interval_temp))
neigh <- 6
clusters_i<-clustITAM(distmat, neigh)
interval_temp$Cluster<-numeric(nrow(interval_temp))
for(j in 1:length(clusters_i)){
#j<-1
for(z in 1:length(clusters_i[[j]])){
#  z<-1
interval_temp$Cluster[as.numeric(clusters_i[[j]][z])]<-j
#df$Clusters[as.numeric(rownames(interval_temp)[nrow(interval_temp)])]<-j
}
}
res[[i]]<-interval_temp
}
// [[Rcpp::export]]
LogicalVector bfs_density_component(NumericMatrix distmat, int root=0, double jump_factor=3, int nbs=10) {
int n = distmat.nrow(); // cantidad de datos
LogicalVector visited(n); // indicara si un nodo esta conectado o no
double infinity=999999999;
double density, sum_local_dists; // empezamos con un valor muy grande para la primera iter
queue<int> aux_queue; // la cola que vamos a usar
int i; // iterador principal
// STEP 0: BEFORE START
for(i=0; i<n; i++){
visited[i] = false;
}
// STEP 1: INITIAL VALUES FOR BREADTH SEARCH
NumericVector distances = distmat.row(root);
NumericVector neighbours(nbs);
distances[root] = infinity;
visited[root] = true;
int n_visited = 0; // para actualizar la densidad
int nbs_queued;
int top=root; // el frente de la cola
aux_queue.push(root);
density = infinity;
// STEP 2: BREADTH-SEARCH
do{
// tomamos el frente de la cola
// cout << "Padre: " << aux_queue.front();
top = aux_queue.front();
aux_queue.pop(); // quitamos el elemento tope
// queremos calcular la distancias para la densidad y los vecinos
distances = distmat.row(top);
distances[top] = infinity;
// vamos a ver que vecinos vamos a visitar
neighbours = smallest_k(distances, nbs);
nbs_queued=0;
sum_local_dists=0;
for(i=0; i < nbs; i++){
// cout << "; vecino " << neighbours[i] << ": " << distances[neighbours[i]] << ", ";
if(!visited[neighbours[i]] && (distances[neighbours[i]] < jump_factor*density) ){
aux_queue.push(neighbours[i]);
visited[neighbours[i]] = true;
sum_local_dists += distances[neighbours[i]];
nbs_queued++;
}
}
density = (density*n_visited + sum_local_dists)/(n_visited + nbs_queued);
n_visited += nbs_queued;
// cout << "densidad: " << density;
// cout << endl;
} while (!aux_queue.empty());
return visited;
}
Rcpp::sourceCpp('Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/git_Denny/TDA_proyectos/proyecto_clustering/graphTheory.cpp')
clustITAM <- function(distmat, neigh){
mat <- distmat
dimnames(mat)=list(1:nrow(distmat),1:nrow(distmat))
clustList <- list()
while(nrow(mat)!=0){
indices <- which(bfs_density_component(mat, nbs=neigh))
if((nrow(mat)-length(indices))!=1){
individuos <- row.names(mat)[indices]
mat <- mat[-indices, -indices]
clustList <- c(clustList, list(individuos))
} else{
el_we_solito <- row.names(mat)[!(row.names(mat) %in% row.names(mat)[indices])]
individuos <- row.names(mat)[indices]
clustList <- c(clustList, list(individuos), list(el_we_solito))
break
}
}
return(clustList)
}
intervals_centers <- seq(min(var_o),max(var_o),length=n_int)  #basic partition = centers
interval_length <- intervals_centers[2]-intervals_centers[1]  #to create the overlaps of p% of this length
intervals <- data.frame(centers=intervals_centers)            #create a data frame
#create the overlapping intervals
intervals$min <- intervals_centers - (0.5+p)*interval_length
intervals$max <- intervals_centers + (0.5+p)*interval_length
intervals$interval <- seq(1,n_int)
intervals$name <- with(intervals, sprintf("[%.2f;%.2f)",min,max))
res <- lapply(split(intervals,intervals$interval), function(x){
return(df[var_o> x$min & var_o <= x$max,])     #res will be a list with each element res[i]
})                                               #being the points on the i'th subset
df$Clusters<-numeric(nrow(df))
for(i in 1:length(res)){
# i<-1
interval_temp<-res[[i]]
distmat <- as.matrix(dist(interval_temp))
neigh <- 6
clusters_i<-clustITAM(distmat, neigh)
interval_temp$Cluster<-numeric(nrow(interval_temp))
for(j in 1:length(clusters_i)){
#j<-1
for(z in 1:length(clusters_i[[j]])){
#  z<-1
interval_temp$Cluster[as.numeric(clusters_i[[j]][z])]<-j
#df$Clusters[as.numeric(rownames(interval_temp)[nrow(interval_temp)])]<-j
}
}
res[[i]]<-interval_temp
}
for(i in 1:length(res)){
df[[ncol(df)+1]]<-numeric(nrow(df))
interval_temp<-res[[i]]
interval_temp$filas<-rownames(interval_temp)
for(j in 1:nrow(df)){
if(rownames(df)[j] %in% rownames(interval_temp)){
df[[ncol(df)]][j]<-interval_temp$Cluster[which(interval_temp$filas==rownames(df)[j])]
}#else{
#       df[[ncol(df)+1]][j]<-0
#     }
}
}
df_<-df
View(df)
for(i in 1:length(res)){
ncol<-ncol(df_)
df_[[ncol+1]]<-numeric(nrow(df_))
ncol<-ncol(df_)
if(i==1){
contador_clusters<-0
df_[[ncol]]<-df_[[2+i]]
}else{
for(j in 1:nrow(df_)){
if(df_[[2+i]][j]!=0){
df_[[ncol]][j]<-df_[[2+i]][j]+contador_clusters
}
}
}
contador_clusters<-contador_clusters+max(df_[[2+i]])
}
View(df_)
df_salvado<-df
df<-df_[,-c(3:8)]
View(df)
View(df_salvado)
df_<-df_salvado
length(res)
View(df_)
for(i in 1:length(res)){
ncol<-ncol(df_)
df_[[ncol+1]]<-numeric(nrow(df_))
ncol<-ncol(df_)
if(i==1){
contador_clusters<-0
df_[[ncol]]<-df_[[2+i]]
}else{
for(j in 1:nrow(df_)){
if(df_[[2+i]][j]!=0){
df_[[ncol]][j]<-df_[[2+i]][j]+contador_clusters
}
}
}
contador_clusters<-contador_clusters+max(df_[[2+i]])
}
View(df_)
df_<-df_salvado
for(i in 1:length(res)){
ncol<-ncol(df_)
df_[[ncol+1]]<-numeric(nrow(df_))
ncol<-ncol(df_)
if(i==1){
contador_clusters<-0
df_[[ncol]]<-df_[[3+i]]
}else{
for(j in 1:nrow(df_)){
if(df_[[3+i]][j]!=0){
df_[[ncol]][j]<-df_[[3+i]][j]+contador_clusters
}
}
}
contador_clusters<-contador_clusters+max(df_[[3+i]])
}
df_salvado<-df
df<-df_[,-c(3:8)]
base <- df
base$clusters<-0
int_fin <- ncol(base)-1
#Columna en donde empieza el intervalo 1:
int_ini <- int_fin-n_int+1
#Columna donde se creo la columna de "clusters":
col_cluster <- ncol(base)
View(df)
View(df_)
for(i in seq(nrow(base[,int_ini:int_fin]))){
temp<-c()
for(m in seq(int_ini,int_fin)){
if (base[i,m] > 0){
temp<-c(paste0(temp,base[i,m],sep = ","))
}
}
if(length((temp))>0){
aux<-unlist(strsplit(temp,","))
aux2<-unique(aux)
aux3<-paste(aux2,collapse=",")
base[i,col_cluster]<-aux3
}
}
base <- data.frame(base$clusters)
names(base) <- c("clusters")
base$obs <- paste("obs",seq(1,length(base$clusters)))
num_clusters <- sort(as.numeric(unique(strsplit(paste0(base$clusters, collapse=","),",")[[1]])))
a<-strsplit(paste0(base$clusters, collapse=","),",")
clusters <- length((num_clusters))
for(x in num_clusters){
base[[paste("c",x,sep="_")]] <- rep(0,nrow(base))
}
base$clusters<- as.character(base$clusters)
for(i in seq(nrow(base))){
vector <- strsplit(base$clusters[i], ",")[[1]]
vector <- sort(as.numeric(vector))
for(x in vector){
base[i,(x+int_ini)] <- 1
}
}
if(sum(base[[3]])==0){
dummy_mat<-base[,4:ncol(base)]
}else{dummy_mat<-base[,3:ncol(base)]}
n_clusters<-ncol(dummy_mat)
cluster_list<-lapply(1:n_clusters,function (col) {which(dummy_mat[,col]==1)})
adj_matrix<-matrix(0,nrow=n_clusters,ncol=n_clusters)
for(i in 1:(n_clusters-1)){
# i<-1
for(j in (i+1):n_clusters){
# j<-i+1
distancia<-setdiff(cluster_list[[i]], cluster_list[[j]])
cercania<-length(cluster_list[[i]])-length(distancia)
adj_matrix[i,j]<-round(cercania/min(length(cluster_list[[i]]),length(cluster_list[[j]])),2)
adj_matrix[j,i]<-adj_matrix[i,j]
}
}
summary_cluster<-matrix(0,nrow=1,ncol=n_clusters)
for(i in 1:n_clusters){
summary_cluster[1,i]<-length(cluster_list[[i]])
}
nodes.n <- clusters
nodes.size<- as.numeric(summary_cluster)/100
nodes.tooltips <- paste("Grupo:", 1:nodes.n)
nodes.names <- 1:nodes.n
nodes.color <- as.character(1:nodes.n)
adj.matrix <- adj_matrix
aux_mat <- data.frame()
for(i in 1:nodes.n) {
for(j in 1:nodes.n){
if(adj.matrix[i, j]!=0) {
aux_mat <- rbind(aux_mat, data.frame(source=i-1, target=j-1, value=adj.matrix[i, j]))
}
}
}
adj.matrix[i, j]
linksJSON <- toJSON(aux_mat)
nodesJSON <- toJSON(data.frame(color=nodes.color, group=nodes.size, name=nodes.names, tooltip=nodes.tooltips))
graphJSON <- sprintf("{\"nodes\": %s, \"links\": %s}", nodesJSON, linksJSON)
htmlFile <- readLines('/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html')
graph_def_line <- which(grepl("graph =", htmlFile))
#htmlFile[graph_def_line] <- sprintf("graph = %s;", graphJSON)
htmlFile[graph_def_line] <- sprintf("graph = %s;", graphJSON)
#writeLines(htmlFile, "www/index.html")
#writeLines(htmlFile, '/home/denny/itam/topologia/ManifoldLearning/www/index.html')
writeLines(htmlFile,'/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html')
browseURL("file:////Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html")
graphJSON <- sprintf("{\"nodes\": %s, \"links\": %s}", nodesJSON, linksJSON)
nodesJSON <- toJSON(data.frame(color=nodes.color, group=nodes.size, name=nodes.names, tooltip=nodes.tooltips))
linksJSON <- toJSON(aux_mat)
nodesJSON <- toJSON(data.frame(color=nodes.color, group=nodes.size, name=nodes.names, tooltip=nodes.tooltips))
length(nodesJSON)
nodesJSON <- toJSON(data.frame(color=nodes.color, group=nodes.size, name=nodes.names, tooltip=nodes.tooltips))
View(summary_cluster)
summary_cluster[,-1]
nodes.size<- as.numeric(summary_cluster[,-1])/100
nodesJSON <- toJSON(data.frame(color=nodes.color, group=nodes.size, name=nodes.names, tooltip=nodes.tooltips))
graphJSON <- sprintf("{\"nodes\": %s, \"links\": %s}", nodesJSON, linksJSON)
htmlFile <- readLines('/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html')
#htmlFile <- readLines("www/index.html")
graph_def_line <- which(grepl("graph =", htmlFile))
#htmlFile[graph_def_line] <- sprintf("graph = %s;", graphJSON)
htmlFile[graph_def_line] <- sprintf("graph = %s;", graphJSON)
#writeLines(htmlFile, "www/index.html")
#writeLines(htmlFile, '/home/denny/itam/topologia/ManifoldLearning/www/index.html')
writeLines(htmlFile,'/Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html')
browseURL("file:////Users/jalfredomb/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Topologia/TDA_proyectos//ManifoldLearning/www/index.html")
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/ComputoParalelo/Comp_Paralelo/ParalelizarMatrizCovarianzas")
generar_matriz(50,10,-2,100)
generar_matriz<-function(filas,columnas,minimo=-1,maximo=50){
#   filas<-3
#   columnas<-5
#   minimo=-1
#   maximo<-50
m<-matrix(cbind(round(runif(filas*columnas,minimo,maximo))),ncol=columnas)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/ComputoParalelo/Comp_Paralelo/ParalelizarMatrizCovarianzas")
out <- file("fuente.csv", "w", encoding="latin1")
write.table(m, out, sep=",", row.names=FALSE,col.names = FALSE)
close(out)
}
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/ComputoParalelo/Comp_Paralelo/ParalelizarMatrizCovarianzas")
generar_matriz(50,10,-2,100)
Matriz<-read.csv("fuente.csv",header = FALSE)
M_covarianzas<-cov(Matriz)*(nrow(Matriz)-1)/nrow(Matriz)
M_covarianzas[ncol(M_covarianzas),ncol(M_covarianzas)]
View(M_covarianzas)
View(Matriz)
generar_matriz(50,500,-2,100)
Matriz<-read.csv("fuente.csv",header = FALSE)
M_covarianzas<-cov(Matriz)*(nrow(Matriz)-1)/nrow(Matriz)
M_covarianzas[ncol(M_covarianzas),ncol(M_covarianzas)]
>>>>>>> 7b263f4b9c1202cc4e925a5b8deb8145883bf996
